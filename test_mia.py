#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ MIA í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
í›ˆë ¨ ê²°ê³¼ê°€ MIA í‰ê°€ì— ì˜ ì—°ê²°ë˜ëŠ”ì§€ í™•ì¸
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
import argparse

def extract_model_info_from_runs(runs_dir):
    """runs ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ"""
    models_info = []
    runs_path = Path(runs_dir)
    
    print(f"ğŸ” Scanning: {runs_dir}")
    
    for method_dir in runs_path.iterdir():
        if not method_dir.is_dir():
            continue
            
        method_name = method_dir.name
        print(f"  Found method: {method_name}")
        
        if method_name == 'dense':
            # Dense: runs/dense/dataset/
            for dataset_dir in method_dir.iterdir():
                if dataset_dir.is_dir():
                    model_info = extract_model_details(dataset_dir, method_name, 0.0)
                    if model_info:
                        models_info.append(model_info)
        
        elif method_name in ['static', 'dpf']:
            # Static/DPF: runs/method/sparsity_X/dataset/
            for sparsity_dir in method_dir.iterdir():
                if sparsity_dir.is_dir() and sparsity_dir.name.startswith('sparsity_'):
                    try:
                        sparsity_str = sparsity_dir.name.replace('sparsity_', '')
                        sparsity = float(sparsity_str)
                    except ValueError:
                        print(f"    Warning: Cannot parse sparsity from {sparsity_dir.name}")
                        continue
                    
                    for dataset_dir in sparsity_dir.iterdir():
                        if dataset_dir.is_dir():
                            model_info = extract_model_details(dataset_dir, method_name, sparsity)
                            if model_info:
                                models_info.append(model_info)
    
    print(f"âœ… Found {len(models_info)} models")
    return models_info

def extract_model_details(model_dir, method, sparsity):
    """ê°œë³„ ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ ì„¸ë¶€ ì •ë³´ ì¶”ì¶œ"""
    
    # í•„ìˆ˜ íŒŒì¼ë“¤ í™•ì¸
    best_model_path = model_dir / 'best_model.pth'
    config_path = model_dir / 'config.json'
    experiment_summary_path = model_dir / 'experiment_summary.json'
    
    if not best_model_path.exists():
        print(f"    âŒ No best_model.pth in {model_dir}")
        return None
    
    model_info = {
        'name': model_dir.parent.name + '_' + model_dir.name,
        'method': method,
        'sparsity': sparsity,
        'path': str(model_dir),
        'files_found': {
            'best_model': best_model_path.exists(),
            'config': config_path.exists(), 
            'summary': experiment_summary_path.exists()
        }
    }
    
    # config.jsonì—ì„œ ì„¤ì • ì •ë³´ ì¶”ì¶œ
    if config_path.exists():
        try:
            with open(config_path) as f:
                config = json.load(f)
            
            model_info.update({
                'dataset': config.get('data', {}).get('dataset', 'unknown'),
                'arch': config.get('model', {}).get('arch', 'unknown'),
                'epochs': config.get('training', {}).get('epochs', 0),
                'actual_method': config.get('pruning', {}).get('method', 'dense'),
                'actual_sparsity': config.get('pruning', {}).get('sparsity', 0.0),
                'pruning_enabled': config.get('pruning', {}).get('enabled', False)
            })
        except Exception as e:
            print(f"    Warning: Error reading config.json: {e}")
    
    # experiment_summary.jsonì—ì„œ ì„±ëŠ¥ ì •ë³´ ì¶”ì¶œ
    if experiment_summary_path.exists():
        try:
            with open(experiment_summary_path) as f:
                summary = json.load(f)
            
            model_info.update({
                'best_acc1': summary.get('best_metrics', {}).get('best_acc1', 0),
                'final_acc1': summary.get('final_metrics', {}).get('acc1', 0),
                'training_time_hours': summary.get('total_duration', 0) / 3600
            })
        except Exception as e:
            print(f"    Warning: Error reading experiment_summary.json: {e}")
    
    print(f"    âœ… {model_info['name']}: {method} (sparsity={sparsity})")
    return model_info

def simulate_simple_mia(models_info):
    """ê°„ë‹¨í•œ MIA ì‹œë®¬ë ˆì´ì…˜ (í…ŒìŠ¤íŠ¸ìš©)"""
    
    mia_results = []
    
    for model_info in models_info:
        # ê°„ë‹¨í•œ confidence-based attack ì‹œë®¬ë ˆì´ì…˜
        # ì‹¤ì œë¡œëŠ” ëª¨ë¸ì„ ë¡œë“œí•´ì„œ predictionì„ ë½‘ì•„ì•¼ í•˜ì§€ë§Œ, 
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ accuracy ê¸°ë°˜ ê°€ìƒ MIA ê²°ê³¼ ìƒì„±
        
        accuracy = model_info.get('best_acc1', 70) / 100.0
        
        # ë†’ì€ ì •í™•ë„ì¼ìˆ˜ë¡ MIAì— ë” ì·¨ì•½í•˜ë‹¤ê³  ê°€ì •
        base_vulnerability = min(0.8, 0.5 + accuracy * 0.3)
        
        # ë°©ë²•ë³„ë¡œ ì·¨ì•½ì„± ì¡°ì •
        if model_info['method'] == 'dense':
            vulnerability_factor = 1.0
        elif model_info['method'] == 'static':
            # Static pruningì€ ì¼ë°˜ì ìœ¼ë¡œ ëœ ì·¨ì•½
            vulnerability_factor = 0.9 - model_info['sparsity'] * 0.1
        else:  # dpf
            # Dynamic pruningì€ ì¤‘ê°„ ìˆ˜ì¤€
            vulnerability_factor = 0.95 - model_info['sparsity'] * 0.05
        
        mia_accuracy = base_vulnerability * vulnerability_factor
        mia_auc = mia_accuracy + np.random.normal(0, 0.05)  # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ
        mia_auc = np.clip(mia_auc, 0.5, 1.0)
        
        mia_result = {
            'experiment': model_info['name'],
            'method': model_info['method'],
            'sparsity': model_info['sparsity'],
            'dataset': model_info.get('dataset', 'cifar10'),
            'best_acc1': model_info.get('best_acc1', 0),
            
            # ê°„ë‹¨í•œ MIA ê²°ê³¼ (ì‹œë®¬ë ˆì´ì…˜)
            'mia_confidence_attack_accuracy': mia_accuracy,
            'mia_confidence_attack_auc': mia_auc,
            'mia_vulnerability_score': mia_accuracy,
            
            # ì¶”ê°€ ì •ë³´
            'files_available': all(model_info['files_found'].values()),
            'path': model_info['path']
        }
        
        mia_results.append(mia_result)
        
        print(f"  ğŸ“Š {model_info['name']}: MIA Attack Acc={mia_accuracy:.3f}, AUC={mia_auc:.3f}")
    
    return mia_results

def create_test_summary(mia_results, output_dir):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(mia_results)
    
    # ì •ë ¬ (methodì™€ sparsityê°€ ìˆëŠ”ì§€ í™•ì¸)
    if 'method' in df.columns and 'sparsity' in df.columns:
        df = df.sort_values(['method', 'sparsity'])
    
    # CSV ì €ì¥
    summary_file = os.path.join(output_dir, 'test_mia_results.csv')
    df.to_csv(summary_file, index=False)
    
    # ìš”ì•½ í†µê³„
    summary_stats = {
        'total_models': len(mia_results),
        'methods': df['method'].value_counts().to_dict() if 'method' in df.columns else {},
        'average_vulnerability': df['mia_vulnerability_score'].mean() if 'mia_vulnerability_score' in df.columns else 0,
        'files_available_count': df['files_available'].sum() if 'files_available' in df.columns else 0
    }
    
    stats_file = os.path.join(output_dir, 'test_summary_stats.json')
    with open(stats_file, 'w') as f:
        json.dump(summary_stats, f, indent=2)
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"  - ì´ ëª¨ë¸ ìˆ˜: {summary_stats['total_models']}")
    print(f"  - ë°©ë²•ë³„ ë¶„í¬: {summary_stats['methods']}")
    print(f"  - í‰ê·  MIA ì·¨ì•½ì„±: {summary_stats['average_vulnerability']:.3f}")
    print(f"  - íŒŒì¼ ì™„ì „í•œ ëª¨ë¸: {summary_stats['files_available_count']}/{summary_stats['total_models']}")
    
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥:")
    print(f"  - ìƒì„¸ ê²°ê³¼: {summary_file}")
    print(f"  - ìš”ì•½ í†µê³„: {stats_file}")
    
    return df, summary_stats

def main():
    parser = argparse.ArgumentParser(description='Test MIA Evaluation')
    parser.add_argument('--runs-dir', default='./runs', help='Directory with trained models')
    parser.add_argument('--output-dir', default='./test_results', help='Output directory')
    
    args = parser.parse_args()
    
    print("ğŸ§ª MIA í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # 1. ëª¨ë¸ ì •ë³´ ì¶”ì¶œ
    models_info = extract_model_info_from_runs(args.runs_dir)
    
    if not models_info:
        print("âŒ í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ê°„ë‹¨í•œ MIA ì‹œë®¬ë ˆì´ì…˜
    print(f"\nğŸ¯ MIA ì‹œë®¬ë ˆì´ì…˜ ({len(models_info)}ê°œ ëª¨ë¸)")
    mia_results = simulate_simple_mia(models_info)
    
    # 3. ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“‹ ê²°ê³¼ ìš”ì•½ ìƒì„±")
    df, stats = create_test_summary(mia_results, args.output_dir)
    
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # 4. ë°ì´í„°í”„ë ˆì„ ë¯¸ë¦¬ë³´ê¸°
    if not df.empty:
        print(f"\nğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        print(df.head().to_string())

if __name__ == '__main__':
    main()