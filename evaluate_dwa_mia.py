#!/usr/bin/env python3
"""
DWA í”„ë£¨ë‹ëœ ëª¨ë¸ì— ëŒ€í•œ MIA í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (WeMeM-main ìŠ¤íƒ€ì¼)
train_dwa.pyë¡œ ìƒì„±ëœ ê²°ê³¼ë¬¼ì„ ê¸°ë°˜ìœ¼ë¡œ MIA ê³µê²© ìˆ˜í–‰

ì‚¬ìš©ë²•:
python evaluate_dwa_mia.py device config_path --dataset_name cifar10 --model_name resnet18 --attacks samia,threshold,nn
"""

import os
import json
import argparse
import pickle
import random
import csv
from pathlib import Path
from datetime import datetime
import numpy as np
import torch
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader, Subset, ConcatDataset

from mia_utils import (
    load_dwa_model, find_dwa_models, get_dataset_loaders,
    create_mia_data_splits, extract_model_features, get_model_sparsity, get_dataset, get_model, get_optimizer, weight_init
)
from attacker_threshold import ThresholdAttacker

def evaluate_single_model(model_info, mia_splits, device='cuda:0', batch_size=128):
    """ë‹¨ì¼ DWA ëª¨ë¸ì— ëŒ€í•œ MIA í‰ê°€"""
    print(f"ğŸ“Š Evaluating: {model_info['dwa_mode']}, {model_info['sparsity_dir']}, {model_info['dataset']}")
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model, config = load_dwa_model(
            model_info['model_path'], 
            model_info['config_path'], 
            device=device
        )
        
        # ì‹¤ì œ sparsity ê³„ì‚°
        actual_sparsity = get_model_sparsity(model)
        
        # ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        dataset_name = config['data']['dataset']
        
        # MIA ë°ì´í„° ì¤€ë¹„ (victim vs shadow + non-member)
        dataset = mia_splits['dataset']
        victim_indices = mia_splits['victim_indices']
        shadow_indices = mia_splits['shadow_indices']
        non_member_indices = mia_splits['non_member_indices']
        
        # Shadowë¥¼ ì ˆë°˜ì”© ë‚˜ëˆ„ì–´ shadow member/non-memberë¡œ ì‚¬ìš©
        half_shadow = len(shadow_indices) // 2
        shadow_member_indices = shadow_indices[:half_shadow]
        shadow_nonmember_indices = shadow_indices[half_shadow:]
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        victim_member_dataset = Subset(dataset, victim_indices[:len(victim_indices)//2])
        victim_nonmember_dataset = Subset(dataset, non_member_indices[:len(victim_indices)//2])
        
        shadow_member_dataset = Subset(dataset, shadow_member_indices)
        shadow_nonmember_dataset = Subset(dataset, shadow_nonmember_indices)
        
        victim_member_loader = DataLoader(victim_member_dataset, batch_size=batch_size, shuffle=False)
        victim_nonmember_loader = DataLoader(victim_nonmember_dataset, batch_size=batch_size, shuffle=False)
        shadow_member_loader = DataLoader(shadow_member_dataset, batch_size=batch_size, shuffle=False)
        shadow_nonmember_loader = DataLoader(shadow_nonmember_dataset, batch_size=batch_size, shuffle=False)
        
        # í”¼ì²˜ ì¶”ì¶œ (static ëª¨ë“œë¡œ í‰ê°€)
        forward_mode = 'static' if config.get('pruning', {}).get('enabled', False) else None
        
        shadow_member_features = extract_model_features(model, shadow_member_loader, device, forward_mode)
        shadow_nonmember_features = extract_model_features(model, shadow_nonmember_loader, device, forward_mode)
        victim_member_features = extract_model_features(model, victim_member_loader, device, forward_mode)
        victim_nonmember_features = extract_model_features(model, victim_nonmember_loader, device, forward_mode)
        
        # Threshold ê¸°ë°˜ ê³µê²© ìˆ˜í–‰
        num_classes = config['data'].get('num_classes', 10 if dataset_name == 'cifar10' else 100)
        
        attacker = ThresholdAttacker(
            in_pair=(shadow_member_features['softmax'], shadow_member_features['targets']),
            out_pair=(shadow_nonmember_features['softmax'], shadow_nonmember_features['targets']),
            v_in_pair=(victim_member_features['softmax'], victim_member_features['targets']),
            v_out_pair=(victim_nonmember_features['softmax'], victim_nonmember_features['targets']),
            num_classes=num_classes
        )
        
        # ê³µê²© ì‹¤í–‰
        conf_acc, entr_acc, mentr_acc = attacker._mem_inf_benchmarks()
        hconf_acc, _, _ = attacker._mem_inf_benchmarks_non_cls()
        
        # ê²°ê³¼ ì •ë¦¬
        result = {
            'experiment_name': config.get('name', 'unknown'),
            'dwa_mode': model_info['dwa_mode'],
            'sparsity_config': config.get('pruning', {}).get('sparsity', 0.0),
            'sparsity_actual': float(actual_sparsity),
            'dataset': dataset_name,
            'architecture': config['model']['arch'],
            'layers': config['model']['layers'],
            'dwa_alpha': config.get('pruning', {}).get('dwa_alpha', 1.0),
            'dwa_beta': config.get('pruning', {}).get('dwa_beta', 1.0),
            'target_epoch': config.get('pruning', {}).get('target_epoch', -1),
            'prune_freq': config.get('pruning', {}).get('prune_freq', -1),
            'attack_conf_gt': float(conf_acc),
            'attack_entropy': float(entr_acc),
            'attack_modified_entropy': float(mentr_acc),
            'attack_conf_top1': float(hconf_acc),
            'victim_member_confidence_mean': float(np.mean(victim_member_features['confidence'])),
            'victim_nonmember_confidence_mean': float(np.mean(victim_nonmember_features['confidence'])),
            'confidence_gap': float(np.mean(victim_member_features['confidence']) - np.mean(victim_nonmember_features['confidence'])),
            'model_path': str(model_info['model_path']),
            'config_path': str(model_info['config_path'])
        }
        
        # ì‹¤í—˜ ìš”ì•½ ì •ë³´ë„ í¬í•¨ (ìˆë‹¤ë©´)
        experiment_summary_path = model_info['experiment_dir'] / 'experiment_summary.json'
        if experiment_summary_path.exists():
            with open(experiment_summary_path) as f:
                exp_summary = json.load(f)
            result['best_acc1'] = exp_summary.get('best_metrics', {}).get('best_acc1', 0.0)
            result['final_acc1'] = exp_summary.get('final_metrics', {}).get('acc1', 0.0)
            result['total_epochs'] = exp_summary.get('total_epochs', 0)
            result['training_time_hours'] = exp_summary.get('total_duration', 0) / 3600
        
        print(f"âœ… Success! Best attack accuracy: {max(conf_acc, entr_acc, mentr_acc, hconf_acc):.3f}")
        return result
        
    except Exception as e:
        print(f"âŒ Failed to evaluate {model_info['model_path']}: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description='DWA MIA Evaluation')
    parser.add_argument('--runs_dir', type=str, default='./runs', help='DWA training results directory')
    parser.add_argument('--output_dir', type=str, default='./mia_results', help='MIA results output directory')
    parser.add_argument('--device', type=str, default='cuda:0', help='Device to use')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for evaluation')
    parser.add_argument('--dataset', type=str, default='cifar10', choices=['cifar10', 'cifar100'], help='Dataset used for training')
    parser.add_argument('--datapath', type=str, default='~/Datasets', help='Dataset path')
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print("ğŸ” DWA ëª¨ë¸ì— ëŒ€í•œ MIA í‰ê°€ ì‹œì‘")
    print("=" * 50)
    
    # DWA í›ˆë ¨ ê²°ê³¼ ëª¨ë¸ë“¤ ì°¾ê¸°
    dwa_models = find_dwa_models(args.runs_dir)
    
    if not dwa_models:
        print(f"âŒ No DWA models found in {args.runs_dir}")
        print("ë¨¼ì € train_dwa.pyë¡œ ëª¨ë¸ì„ í›ˆë ¨í•´ì£¼ì„¸ìš”.")
        return
    
    print(f"ğŸ“‹ Found {len(dwa_models)} DWA models")
    
    # MIA ë°ì´í„° ë¶„í•  ì¤€ë¹„ (í•œ ë²ˆë§Œ ìƒì„±)
    print("ğŸ”„ Preparing MIA data splits...")
    mia_splits = create_mia_data_splits(args.dataset)
    
    # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ MIA í‰ê°€ ìˆ˜í–‰
    all_results = []
    failed_count = 0
    
    for i, model_info in enumerate(dwa_models, 1):
        print(f"\nğŸ“Š [{i}/{len(dwa_models)}] Evaluating model...")
        
        result = evaluate_single_model(
            model_info, mia_splits, 
            device=args.device, batch_size=args.batch_size
        )
        
        if result:
            all_results.append(result)
            
            # ê°œë³„ ê²°ê³¼ ì €ì¥
            individual_result_path = model_info['experiment_dir'] / 'mia_evaluation.json'
            with open(individual_result_path, 'w') as f:
                json.dump(result, f, indent=2)
        else:
            failed_count += 1
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    if all_results:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ì €ì¥
        json_path = output_dir / f'dwa_mia_results_{timestamp}.json'
        with open(json_path, 'w') as f:
            json.dump(all_results, f, indent=2)
        
        # CSV ì €ì¥
        csv_path = output_dir / f'dwa_mia_results_{timestamp}.csv'
        fieldnames = list(all_results[0].keys())
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_results)
        
        print(f"\nğŸ’¾ Results saved:")
        print(f"   JSON: {json_path}")
        print(f"   CSV:  {csv_path}")
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“ˆ MIA Attack Success Summary:")
        print(f"{'DWA Mode':<20} {'Sparsity':<10} {'Best Attack':<12} {'Conf Gap':<12}")
        print("-" * 60)
        
        # ì •ë ¬í•´ì„œ ì¶œë ¥
        sorted_results = sorted(all_results, key=lambda x: (x['dwa_mode'], x['sparsity_actual']))
        
        for result in sorted_results:
            best_attack = max(
                result['attack_conf_gt'], 
                result['attack_entropy'], 
                result['attack_modified_entropy'],
                result['attack_conf_top1']
            )
            print(f"{result['dwa_mode']:<20} {result['sparsity_actual']:<10.3f} {best_attack:<12.3f} {result['confidence_gap']:<12.3f}")
        
        print(f"\nâœ… Successfully evaluated {len(all_results)} models")
        if failed_count > 0:
            print(f"âŒ Failed to evaluate {failed_count} models")
    
    else:
        print("âŒ No successful evaluations")

if __name__ == '__main__':
    main()