#!/usr/bin/env python3
"""
MIA ì‹¤í—˜ìš© ë°ì´í„° ë¶„í•  ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸
train_dwa.py ì‹¤í–‰ ì „ì— MIAìš© ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¶„í• í•´ë‘ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
python scripts/prepare_mia_data.py --dataset cifar10 --output_dir ./mia_data
"""

import os
import sys
import argparse
import random
import pickle
import numpy as np
import torch
from pathlib import Path
from torch.utils.data import Subset

# ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from data import cifar10_loader, cifar100_loader

def create_mia_data_splits(dataset_name, victim_ratio=0.4, shadow_ratio=0.4, test_ratio=0.1, seed=42):
    """
    MIAìš© ë°ì´í„° ë¶„í•  ìƒì„±
    
    Args:
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ ('cifar10', 'cifar100')
        victim_ratio: victim ëª¨ë¸ í›ˆë ¨ìš© ë¹„ìœ¨
        shadow_ratio: shadow ëª¨ë¸ í›ˆë ¨ìš© ë¹„ìœ¨  
        test_ratio: ìµœì¢… í…ŒìŠ¤íŠ¸ìš© ë¹„ìœ¨
        seed: ëœë¤ ì‹œë“œ
    
    Returns:
        dict: ë¶„í• ëœ ì¸ë±ìŠ¤ë“¤ê³¼ ë©”íƒ€ì •ë³´
    """
    print(f"ğŸ”„ Creating MIA data splits for {dataset_name}")
    
    # ì‹œë“œ ì„¤ì •
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_name == 'cifar10':
        train_loader, val_loader = cifar10_loader(1, 0, '~/Datasets/CIFAR', cuda=False)
        num_classes = 10
    elif dataset_name == 'cifar100':
        train_loader, val_loader = cifar100_loader(1, 0, '~/Datasets/CIFAR', cuda=False) 
        num_classes = 100
    else:
        raise ValueError(f"Unsupported dataset: {dataset_name}")
    
    train_dataset = train_loader.dataset
    val_dataset = val_loader.dataset
    
    print(f"ğŸ“Š Dataset info: Train={len(train_dataset)}, Val={len(val_dataset)}, Classes={num_classes}")
    
    # í›ˆë ¨ ë°ì´í„° ì¸ë±ìŠ¤ ì„ê¸°
    total_size = len(train_dataset)
    indices = list(range(total_size))
    random.shuffle(indices)
    
    # ë¹„ìœ¨ì— ë”°ë¼ ë¶„í• 
    victim_size = int(total_size * victim_ratio)
    shadow_size = int(total_size * shadow_ratio)
    test_size = int(total_size * test_ratio)
    nonmember_size = total_size - victim_size - shadow_size - test_size
    
    # ì¸ë±ìŠ¤ ë¶„í• 
    victim_indices = indices[:victim_size]
    shadow_indices = indices[victim_size:victim_size + shadow_size] 
    test_indices = indices[victim_size + shadow_size:victim_size + shadow_size + test_size]
    nonmember_indices = indices[victim_size + shadow_size + test_size:]
    
    # í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
    def get_class_distribution(dataset, indices):
        targets = [dataset.targets[i] for i in indices]
        unique, counts = np.unique(targets, return_counts=True)
        return dict(zip(unique.tolist(), counts.tolist()))
    
    victim_dist = get_class_distribution(train_dataset, victim_indices)
    shadow_dist = get_class_distribution(train_dataset, shadow_indices)
    
    splits = {
        'dataset_name': dataset_name,
        'num_classes': num_classes,
        'total_size': total_size,
        'seed': seed,
        'ratios': {
            'victim': victim_ratio,
            'shadow': shadow_ratio, 
            'test': test_ratio,
            'nonmember': nonmember_size / total_size
        },
        'splits': {
            'victim_indices': victim_indices,
            'shadow_indices': shadow_indices,
            'test_indices': test_indices, 
            'nonmember_indices': nonmember_indices
        },
        'class_distributions': {
            'victim': victim_dist,
            'shadow': shadow_dist
        },
        'split_sizes': {
            'victim': len(victim_indices),
            'shadow': len(shadow_indices),
            'test': len(test_indices),
            'nonmember': len(nonmember_indices)
        }
    }
    
    print(f"âœ… Split sizes: Victim={len(victim_indices)}, Shadow={len(shadow_indices)}, Test={len(test_indices)}, NonMember={len(nonmember_indices)}")
    
    return splits

def save_mia_splits(splits, output_dir):
    """MIA ë¶„í•  ì €ì¥"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    dataset_name = splits['dataset_name']
    
    # ë©”ì¸ ë¶„í•  íŒŒì¼ ì €ì¥
    split_file = output_dir / f'{dataset_name}_mia_splits.pkl'
    with open(split_file, 'wb') as f:
        pickle.dump(splits, f)
    
    # ë©”íƒ€ì •ë³´ JSONìœ¼ë¡œë„ ì €ì¥
    import json
    meta_file = output_dir / f'{dataset_name}_mia_splits_info.json'
    
    # pickle ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ë¶€ë¶„ ì œì™¸í•˜ê³  ì €ì¥
    meta_info = {
        'dataset_name': splits['dataset_name'],
        'num_classes': splits['num_classes'],
        'total_size': splits['total_size'], 
        'seed': splits['seed'],
        'ratios': splits['ratios'],
        'split_sizes': splits['split_sizes'],
        'class_distributions': splits['class_distributions']
    }
    
    with open(meta_file, 'w') as f:
        json.dump(meta_info, f, indent=2)
    
    print(f"ğŸ’¾ Saved MIA splits:")
    print(f"   Data: {split_file}")
    print(f"   Info: {meta_file}")
    
    return split_file

def load_mia_splits(dataset_name, output_dir='./mia_data'):
    """ì €ì¥ëœ MIA ë¶„í•  ë¡œë“œ"""
    output_dir = Path(output_dir) 
    split_file = output_dir / f'{dataset_name}_mia_splits.pkl'
    
    if not split_file.exists():
        raise FileNotFoundError(f"MIA splits not found: {split_file}")
    
    with open(split_file, 'rb') as f:
        splits = pickle.load(f)
    
    print(f"ğŸ“‚ Loaded MIA splits from {split_file}")
    return splits

def verify_splits(splits, dataset_name):
    """ë¶„í•  ê²€ì¦"""
    print(f"\nğŸ” Verifying splits for {dataset_name}...")
    
    # ë°ì´í„°ì…‹ ë‹¤ì‹œ ë¡œë“œí•´ì„œ ê²€ì¦
    if dataset_name == 'cifar10':
        train_loader, _ = cifar10_loader(1, 0, '~/Datasets/CIFAR', cuda=False)
    elif dataset_name == 'cifar100':
        train_loader, _ = cifar100_loader(1, 0, '~/Datasets/CIFAR', cuda=False)
    else:
        return False
        
    dataset = train_loader.dataset
    
    # ì¸ë±ìŠ¤ ì¤‘ë³µ í™•ì¸
    all_indices = set()
    for split_name in ['victim_indices', 'shadow_indices', 'test_indices', 'nonmember_indices']:
        indices = splits['splits'][split_name]
        if all_indices & set(indices):
            print(f"âŒ Overlap found in {split_name}")
            return False
        all_indices.update(indices)
    
    # ì „ì²´ í¬ê¸° í™•ì¸
    if len(all_indices) != len(dataset):
        print(f"âŒ Size mismatch: expected {len(dataset)}, got {len(all_indices)}")
        return False
    
    print("âœ… All splits are valid!")
    return True

def main():
    parser = argparse.ArgumentParser(description='Prepare MIA data splits')
    parser.add_argument('--dataset', type=str, default='cifar10', 
                       choices=['cifar10', 'cifar100'], help='Dataset name')
    parser.add_argument('--output_dir', type=str, default='./mia_data', 
                       help='Output directory for MIA splits') 
    parser.add_argument('--victim_ratio', type=float, default=0.4,
                       help='Ratio for victim training data')
    parser.add_argument('--shadow_ratio', type=float, default=0.4, 
                       help='Ratio for shadow training data')
    parser.add_argument('--test_ratio', type=float, default=0.1,
                       help='Ratio for test data')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--verify_only', action='store_true', 
                       help='Only verify existing splits')
    
    args = parser.parse_args()
    
    if args.verify_only:
        # ê¸°ì¡´ ë¶„í• ë§Œ ê²€ì¦
        try:
            splits = load_mia_splits(args.dataset, args.output_dir)
            verify_splits(splits, args.dataset)
        except FileNotFoundError:
            print(f"âŒ No existing splits found for {args.dataset}")
        return
    
    print("ğŸš€ Preparing MIA data splits...")
    print("=" * 50)
    
    # ë¹„ìœ¨ ê²€ì¦
    total_ratio = args.victim_ratio + args.shadow_ratio + args.test_ratio
    if total_ratio > 1.0:
        print(f"âŒ Total ratio ({total_ratio}) exceeds 1.0")
        return
    
    nonmember_ratio = 1.0 - total_ratio
    print(f"ğŸ“Š Split ratios: Victim={args.victim_ratio}, Shadow={args.shadow_ratio}, Test={args.test_ratio}, NonMember={nonmember_ratio:.3f}")
    
    # ë¶„í•  ìƒì„±
    splits = create_mia_data_splits(
        args.dataset,
        victim_ratio=args.victim_ratio,
        shadow_ratio=args.shadow_ratio, 
        test_ratio=args.test_ratio,
        seed=args.seed
    )
    
    # ì €ì¥
    split_file = save_mia_splits(splits, args.output_dir)
    
    # ê²€ì¦
    if verify_splits(splits, args.dataset):
        print("\nâœ… MIA data splits prepared successfully!")
        print(f"ğŸ“ Use these splits in your MIA evaluation with: --mia_splits {split_file}")
    else:
        print("\nâŒ Split verification failed!")

if __name__ == '__main__':
    main()