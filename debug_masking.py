#!/usr/bin/env python3
"""
MaskConv2d ë™ì‘ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
type_value ì„¤ì •ê³¼ ê·¸ë¼ë””ì–¸íŠ¸ ì „íŒŒë¥¼ í™•ì¸
"""

import torch
import torch.nn as nn
import sys
import os

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pruning.dcil.mnn import MaskConv2d, MaskerDynamic, MaskerStatic

def test_masker_behavior():
    """MaskerDynamic vs MaskerStatic ê·¸ë¼ë””ì–¸íŠ¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” Masker ê·¸ë¼ë””ì–¸íŠ¸ ë™ì‘ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    w = torch.nn.Parameter(torch.ones(4, requires_grad=True))
    mask = torch.tensor([1., 0., 1., 0.])  # 0ë²ˆ, 2ë²ˆë§Œ í™œì„±í™”
    
    print(f"ì›ë³¸ weight: {w.data}")
    print(f"mask: {mask}")
    
    # 1. Dynamic í…ŒìŠ¤íŠ¸ (ì¬í™œì„±í™” ê°€ëŠ¥)
    print("\n--- MaskerDynamic (DPF) ---")
    w.grad = None
    y_dyn = MaskerDynamic.apply(w, mask).sum()
    y_dyn.backward(retain_graph=True)
    print(f"Dynamic grad: {w.grad}")
    print(f"ë§ˆìŠ¤í¬=0 ìœ„ì¹˜ grad ê°’: {w.grad[1].item():.6f}, {w.grad[3].item():.6f}")
    
    # 2. Static í…ŒìŠ¤íŠ¸ (ì¬í™œì„±í™” ë¶ˆê°€)
    print("\n--- MaskerStatic (Static Pruning) ---")
    w.grad = None
    y_sta = MaskerStatic.apply(w, mask).sum()
    y_sta.backward()
    print(f"Static grad: {w.grad}")
    print(f"ë§ˆìŠ¤í¬=0 ìœ„ì¹˜ grad ê°’: {w.grad[1].item():.6f}, {w.grad[3].item():.6f}")

def test_maskconv2d_typevalue():
    """MaskConv2dì˜ type_valueë³„ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª MaskConv2d type_value í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # ê°„ë‹¨í•œ Conv ë ˆì´ì–´ ìƒì„±
    conv = MaskConv2d(3, 16, 3, padding=1)
    
    # ë§ˆìŠ¤í¬ ì„¤ì • (ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ)
    with torch.no_grad():
        conv.mask.data[:8] = 0  # ì²˜ìŒ 8ê°œ í•„í„° ë§ˆìŠ¤í‚¹
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    x = torch.randn(1, 3, 32, 32, requires_grad=True)
    
    print(f"ë§ˆìŠ¤í‚¹ëœ ê°€ì¤‘ì¹˜ ë¹„ìœ¨: {(conv.mask == 0).sum().item() / conv.mask.numel():.2%}")
    
    # ê° type_valueë³„ í…ŒìŠ¤íŠ¸
    for type_val, name in [(0, "ê¸°ë³¸(Static)"), (5, "MaskerStatic"), (6, "MaskerDynamic")]:
        print(f"\n--- type_value={type_val} ({name}) ---")
        
        conv.type_value = type_val
        conv.zero_grad()
        if x.grad is not None:
            x.grad.zero_()
        
        y = conv(x)
        loss = y.sum()
        loss.backward(retain_graph=True)
        
        # ë§ˆìŠ¤í‚¹ëœ ê°€ì¤‘ì¹˜ì˜ ê·¸ë¼ë””ì–¸íŠ¸ í™•ì¸
        masked_grad = conv.weight.grad[conv.mask == 0]
        active_grad = conv.weight.grad[conv.mask == 1]
        
        print(f"ë§ˆìŠ¤í‚¹ëœ ê°€ì¤‘ì¹˜ grad í•©ê³„: {masked_grad.sum().item():.6f}")
        print(f"í™œì„± ê°€ì¤‘ì¹˜ grad í•©ê³„: {active_grad.sum().item():.6f}")
        print(f"ë§ˆìŠ¤í‚¹ëœ ê°€ì¤‘ì¹˜ grad != 0 ê°œìˆ˜: {(masked_grad != 0).sum().item()}")

def create_training_monitor():
    """í›ˆë ¨ ì¤‘ ëª¨ë‹ˆí„°ë§ìš© ì½”ë“œ ìƒì„±"""
    monitor_code = '''
# í›ˆë ¨ ë£¨í”„ì— ì¶”ê°€í•  ëª¨ë‹ˆí„°ë§ ì½”ë“œ
def monitor_masking_behavior(model, epoch, iteration):
    """ëª¨ë¸ì˜ ë§ˆìŠ¤í‚¹ ë™ì‘ ëª¨ë‹ˆí„°ë§"""
    if iteration % 100 == 0:  # 100 iterationë§ˆë‹¤ ì²´í¬
        mask_layers = []
        for name, module in model.named_modules():
            if isinstance(module, MaskConv2d):
                mask_layers.append({
                    'name': name,
                    'type_value': module.type_value,
                    'mask_sparsity': (module.mask == 0).float().mean().item(),
                    'weight_grad_nonzero': (module.weight.grad != 0).float().mean().item() if module.weight.grad is not None else 0
                })
        
        if mask_layers:
            print(f"\\n[Epoch {epoch}, Iter {iteration}] Masking Status:")
            for layer_info in mask_layers[:3]:  # ì²˜ìŒ 3ê°œ ë ˆì´ì–´ë§Œ ì¶œë ¥
                print(f"  {layer_info['name']}: type_value={layer_info['type_value']}, "
                      f"sparsity={layer_info['mask_sparsity']:.3f}, "
                      f"grad_nonzero={layer_info['weight_grad_nonzero']:.3f}")

# run_experiment.pyì˜ train_one_epoch í•¨ìˆ˜ì— ì¶”ê°€:
# monitor_masking_behavior(model, epoch, iteration)
'''
    
    print("\nğŸ“ í›ˆë ¨ ëª¨ë‹ˆí„°ë§ ì½”ë“œ")
    print("=" * 40)
    print(monitor_code)
    
    return monitor_code

if __name__ == "__main__":
    print("ğŸš€ MaskConv2d ë™ì‘ ê²€ì¦ ì‹œì‘")
    print("=" * 50)
    
    # 1. ê¸°ë³¸ Masker ë™ì‘ í…ŒìŠ¤íŠ¸
    test_masker_behavior()
    
    # 2. MaskConv2d type_value í…ŒìŠ¤íŠ¸
    test_maskconv2d_typevalue()
    
    # 3. í›ˆë ¨ ëª¨ë‹ˆí„°ë§ ì½”ë“œ ìƒì„±
    create_training_monitor()
    
    print(f"\nâœ… ê²€ì¦ ì™„ë£Œ!")
    print(f"ğŸ’¡ í›ˆë ¨ ì¤‘ ëª¨ë‹ˆí„°ë§ì„ ì›í•œë‹¤ë©´ ìœ„ì˜ monitor_masking_behavior í•¨ìˆ˜ë¥¼ run_experiment.pyì— ì¶”ê°€í•˜ì„¸ìš”.")