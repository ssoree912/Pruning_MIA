#!/usr/bin/env python3
"""
MaskConv2d ë™ì‘ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
type_value ì„¤ì •ê³¼ ê·¸ë¼ë””ì–¸íŠ¸ ì „íŒŒë¥¼ í™•ì¸
"""

import torch
import torch.nn as nn
import sys
import os

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pruning.dcil.mnn import MaskConv2d, MaskerDynamic, MaskerStatic

def test_masker_behavior():
    """MaskerDynamic vs MaskerStatic ê·¸ë¼ë””ì–¸íŠ¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” Masker ê·¸ë¼ë””ì–¸íŠ¸ ë™ì‘ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    w = torch.nn.Parameter(torch.ones(4, requires_grad=True))
    mask = torch.tensor([1., 0., 1., 0.])  # 0ë²ˆ, 2ë²ˆë§Œ í™œì„±í™”
    
    print(f"ì›ë³¸ weight: {w.data}")
    print(f"mask: {mask}")
    
    # 1. Dynamic í…ŒìŠ¤íŠ¸ (ì¬í™œì„±í™” ê°€ëŠ¥)
    print("\n--- MaskerDynamic (DPF) ---")
    w.grad = None
    y_dyn = MaskerDynamic.apply(w, mask).sum()
    y_dyn.backward(retain_graph=True)
    print(f"Dynamic grad: {w.grad}")
    print(f"ë§ˆìŠ¤í¬=0 ìœ„ì¹˜ grad ê°’: {w.grad[1].item():.6f}, {w.grad[3].item():.6f}")
    
    # 2. Static í…ŒìŠ¤íŠ¸ (ì¬í™œì„±í™” ë¶ˆê°€)
    print("\n--- MaskerStatic (Static Pruning) ---")
    w.grad = None
    y_sta = MaskerStatic.apply(w, mask).sum()
    y_sta.backward()
    print(f"Static grad: {w.grad}")
    print(f"ë§ˆìŠ¤í¬=0 ìœ„ì¹˜ grad ê°’: {w.grad[1].item():.6f}, {w.grad[3].item():.6f}")

def test_maskconv2d_typevalue():
    """MaskConv2dì˜ type_valueë³„ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª MaskConv2d type_value í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # ê°„ë‹¨í•œ Conv ë ˆì´ì–´ ìƒì„±
    conv = MaskConv2d(3, 16, 3, padding=1)
    
    # ë§ˆìŠ¤í¬ ì„¤ì • (ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ)
    with torch.no_grad():
        conv.mask.data[:8] = 0  # ì²˜ìŒ 8ê°œ í•„í„° ë§ˆìŠ¤í‚¹
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    x = torch.randn(1, 3, 32, 32, requires_grad=True)
    
    print(f"ë§ˆìŠ¤í‚¹ëœ ê°€ì¤‘ì¹˜ ë¹„ìœ¨: {(conv.mask == 0).sum().item() / conv.mask.numel():.2%}")
    
    # ê° type_valueë³„ í…ŒìŠ¤íŠ¸
    for type_val, name in [(0, "ê¸°ë³¸(Static)"), (5, "MaskerStatic"), (6, "MaskerDynamic")]:
        print(f"\n--- type_value={type_val} ({name}) ---")
        
        conv.type_value = type_val
        conv.zero_grad()
        if x.grad is not None:
            x.grad.zero_()
        
        y = conv(x)
        loss = y.sum()
        loss.backward(retain_graph=True)
        
        # ë§ˆìŠ¤í‚¹ëœ ê°€ì¤‘ì¹˜ì˜ ê·¸ë¼ë””ì–¸íŠ¸ í™•ì¸
        masked_grad = conv.weight.grad[conv.mask == 0]
        active_grad = conv.weight.grad[conv.mask == 1]
        
        print(f"ë§ˆìŠ¤í‚¹ëœ ê°€ì¤‘ì¹˜ grad í•©ê³„: {masked_grad.sum().item():.6f}")
        print(f"í™œì„± ê°€ì¤‘ì¹˜ grad í•©ê³„: {active_grad.sum().item():.6f}")
        print(f"ë§ˆìŠ¤í‚¹ëœ ê°€ì¤‘ì¹˜ grad != 0 ê°œìˆ˜: {(masked_grad != 0).sum().item()}")

def create_training_monitor():
    """í›ˆë ¨ ì¤‘ ëª¨ë‹ˆí„°ë§ìš© ì½”ë“œ ìƒì„±"""
    monitor_code = '''
# í›ˆë ¨ ë£¨í”„ì— ì¶”ê°€í•  ëª¨ë‹ˆí„°ë§ ì½”ë“œ(êµì²´ ë²„ì „)
def monitor_masking_behavior(model, epoch, iteration, max_layers=3):
    net = model.module if hasattr(model, "module") else model
    if iteration % 100 == 0:
        printed = 0
        print(f"\n[Epoch {epoch}, Iter {iteration}] Masking Status:")
        for name, module in net.named_modules():
            if isinstance(module, MaskConv2d):
                mask = module.mask
                sparsity = (mask == 0).float().mean().item()
                tv = getattr(module, "type_value", None)
                if module.weight.grad is not None:
                    g = module.weight.grad
                    masked = (mask == 0)
                    active = (mask == 1)
                    # ë¶„ë¦¬ëœ grad í†µê³„
                    masked_nonzero = (g[masked] != 0).float().mean().item() if masked.any() else 0.0
                    active_nonzero = (g[active] != 0).float().mean().item() if active.any() else 0.0
                    masked_sum = g[masked].abs().sum().item() if masked.any() else 0.0
                    active_sum = g[active].abs().sum().item() if active.any() else 0.0
                else:
                    masked_nonzero = active_nonzero = masked_sum = active_sum = 0.0

                print(f"  {name}: type_value={tv}, sparsity={sparsity:.3f}, "
                      f"masked_grad_nonzero={masked_nonzero:.3f}, active_grad_nonzero={active_nonzero:.3f}, "
                      f"masked_grad_sum={masked_sum:.2e}, active_grad_sum={active_sum:.2e}")
                printed += 1
                if printed >= max_layers:
                    break
'''
    
    print("\nğŸ“ í›ˆë ¨ ëª¨ë‹ˆí„°ë§ ì½”ë“œ")
    print("=" * 40)
    print(monitor_code)
    
    return monitor_code

if __name__ == "__main__":
    print("ğŸš€ MaskConv2d ë™ì‘ ê²€ì¦ ì‹œì‘")
    print("=" * 50)
    
    # 1. ê¸°ë³¸ Masker ë™ì‘ í…ŒìŠ¤íŠ¸
    test_masker_behavior()
    
    # 2. MaskConv2d type_value í…ŒìŠ¤íŠ¸
    test_maskconv2d_typevalue()
    
    # 3. í›ˆë ¨ ëª¨ë‹ˆí„°ë§ ì½”ë“œ ìƒì„±
    create_training_monitor()
    
    print(f"\nâœ… ê²€ì¦ ì™„ë£Œ!")
    print(f"ğŸ’¡ í›ˆë ¨ ì¤‘ ëª¨ë‹ˆí„°ë§ì„ ì›í•œë‹¤ë©´ ìœ„ì˜ monitor_masking_behavior í•¨ìˆ˜ë¥¼ run_experiment.pyì— ì¶”ê°€í•˜ì„¸ìš”.")