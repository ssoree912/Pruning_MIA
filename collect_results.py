#!/usr/bin/env python3
"""
í›ˆë ¨ ê²°ê³¼ ìˆ˜ì§‘ ë° ì •í™•ë„ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
import pandas as pd
from pathlib import Path

def collect_training_results():
    """ì‹¤ì œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ í›ˆë ¨ ê²°ê³¼ ìˆ˜ì§‘: runs/method/sparsity/seed/"""
    
    runs_dir = Path('./runs')
    results = []
    
    print("ğŸ“Š í›ˆë ¨ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...")
    print(f"ê²€ìƒ‰ ë””ë ‰í† ë¦¬: {runs_dir.absolute()}")
    
    # ì‹¤ì œ êµ¬ì¡°: runs/method/sparsity/seed/
    for method_dir in runs_dir.iterdir():
        if not method_dir.is_dir() or method_dir.name == 'final_report':
            continue
        
        print(f"  ë°©ë²•: {method_dir.name}")
        
        for sparsity_dir in method_dir.iterdir():
            if not sparsity_dir.is_dir():
                continue
                
            for seed_dir in sparsity_dir.iterdir():
                if not seed_dir.is_dir():
                    continue
                    
                config_path = seed_dir / 'config.json'
                summary_path = seed_dir / 'experiment_summary.json'
                
                if config_path.exists() and summary_path.exists():
                    try:
                        with open(config_path) as f:
                            config = json.load(f)
                        
                        with open(summary_path) as f:
                            summary = json.load(f)
                        
                        # ë°©ë²• íŒŒì‹±
                        method = method_dir.name
                        
                        # ìŠ¤íŒŒì‹œí‹° íŒŒì‹± 
                        if method == 'dense':
                            sparsity = 0.0
                        else:
                            sparsity = config.get('pruning', {}).get('sparsity', 0.0)
                        
                        # ì •í™•ë„ í•„ë“œ í™•ì¸ (ì—¬ëŸ¬ ê°€ëŠ¥ì„±)
                        best_acc = (summary.get('best_acc1', 0.0) or 
                                   summary.get('acc1', 0.0) or 
                                   summary.get('accuracy', 0.0))
                        
                        final_acc = (summary.get('final_acc1', 0.0) or
                                    summary.get('acc1', 0.0))
                        
                        # ì‹œê°„ ê³„ì‚° (ì´ˆ â†’ ì‹œê°„)
                        duration = summary.get('total_duration', 0.0)
                        if duration > 100:  # ì´ˆ ë‹¨ìœ„ì¸ ê²½ìš°
                            duration_hours = duration / 3600
                        else:
                            duration_hours = summary.get('total_duration_hours', 0.0)
                        
                        result = {
                            'name': config['name'],
                            'method': method,
                            'sparsity': sparsity,
                            'sparsity_percent': sparsity * 100,
                            'best_acc1': best_acc,
                            'final_acc1': final_acc,
                            'total_duration_hours': duration_hours,
                            'epochs': config.get('training', {}).get('epochs', 0),
                            'seed': config.get('system', {}).get('seed', 42)
                        }
                        
                        results.append(result)
                        print(f"    âœ“ {config['name']}: {best_acc:.2f}%")
                        
                    except Exception as e:
                        print(f"    âœ— Error processing {seed_dir}: {e}")
                else:
                    print(f"    âš ï¸ Missing files in {seed_dir}")
    
    return results

def create_summary_report(results):
    """ì •í™•ë„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    if not results:
        print("âš ï¸ í›ˆë ¨ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df = pd.DataFrame(results)
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('./runs/final_report', exist_ok=True)
    
    # CSV ì €ì¥
    df.to_csv('./runs/final_report/experiments_comparison.csv', index=False)
    
    print(f"\nâœ… ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ: {len(results)}ê°œ ëª¨ë¸")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: ./runs/final_report/experiments_comparison.csv")
    
    # ì •í™•ë„ ìˆœìœ„ ì¶œë ¥
    print("\nğŸ† ì •í™•ë„ ìˆœìœ„ (Top 10):")
    print("=" * 50)
    
    df_sorted = df.sort_values('best_acc1', ascending=False)
    for i, (_, row) in enumerate(df_sorted.head(10).iterrows(), 1):
        sparsity_text = f"{row.sparsity_percent:.0f}%" if row.sparsity > 0 else "0%"
        method_text = row.method.upper()
        print(f"{i:2d}. {row.name:<25} {row.best_acc1:6.2f}% ({method_text} {sparsity_text})")
    
    # ë°©ë²•ë³„ í†µê³„
    print("\nğŸ“Š ë°©ë²•ë³„ ì„±ëŠ¥ í†µê³„:")
    print("=" * 50)
    
    methods = ['dense', 'static', 'dpf']
    for method in methods:
        method_data = df[df['method'] == method]
        if len(method_data) > 0:
            print(f"\n{method.upper()}:")
            print(f"  ëª¨ë¸ ìˆ˜: {len(method_data)}ê°œ")
            print(f"  í‰ê·  ì •í™•ë„: {method_data['best_acc1'].mean():.2f}% Â± {method_data['best_acc1'].std():.2f}%")
            print(f"  ìµœê³  ì •í™•ë„: {method_data['best_acc1'].max():.2f}%")
            print(f"  ìµœì € ì •í™•ë„: {method_data['best_acc1'].min():.2f}%")
            print(f"  í‰ê·  í›ˆë ¨ì‹œê°„: {method_data['total_duration_hours'].mean():.2f}ì‹œê°„")
    
    # ìŠ¤íŒŒì‹œí‹°ë³„ ë¶„ì„
    print("\nğŸ“ˆ ìŠ¤íŒŒì‹œí‹°ë³„ ì„±ëŠ¥ ë¹„êµ:")
    print("=" * 50)
    
    sparsities = sorted(df[df['sparsity'] > 0]['sparsity_percent'].unique())
    
    if len(sparsities) > 0:
        print(f"{'ìŠ¤íŒŒì‹œí‹°':<8} {'Static':<8} {'DPF':<8} {'ì°¨ì´':<8}")
        print("-" * 32)
        
        for sparsity in sparsities:
            static_data = df[(df['method'] == 'static') & (df['sparsity_percent'] == sparsity)]
            dpf_data = df[(df['method'] == 'dpf') & (df['sparsity_percent'] == sparsity)]
            
            static_acc = static_data['best_acc1'].mean() if len(static_data) > 0 else 0
            dpf_acc = dpf_data['best_acc1'].mean() if len(dpf_data) > 0 else 0
            diff = dpf_acc - static_acc
            
            print(f"{sparsity:6.0f}%   {static_acc:6.2f}%  {dpf_acc:6.2f}%  {diff:+6.2f}%")
    
    print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ CSV: ./runs/final_report/experiments_comparison.csv")

def main():
    print("ğŸ¯ Dense vs Static vs DPF í›ˆë ¨ ê²°ê³¼ ì •ë¦¬")
    print("=" * 50)
    
    # ê²°ê³¼ ìˆ˜ì§‘
    results = collect_training_results()
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    create_summary_report(results)
    
    print("\nğŸ ê²°ê³¼ ì •ë¦¬ ì™„ë£Œ!")

if __name__ == '__main__':
    main()