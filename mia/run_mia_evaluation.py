#!/usr/bin/env python3
"""
Dense/Static/DPF ëª¨ë¸ì— ëŒ€í•œ MIA í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
import torch
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader, Dataset
import sys
import argparse
import re

# Add current directory to path
sys.path.append('.')

from data import cifar10_loader
from models.resnet import resnet

class MIADataset(Dataset):
    """MIAìš© ë°ì´í„°ì…‹"""
    def __init__(self, data, targets, transform=None):
        self.data = data
        self.targets = targets
        self.transform = transform
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x, y = self.data[idx], self.targets[idx]
        if self.transform:
            x = self.transform(x)
        return x, y

def load_model_from_checkpoint(model_path, config_path):
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ (ì„¤ì • íŒŒì¼ ì°¸ì¡°)"""
    
    # ì„¤ì • íŒŒì¼ì—ì„œ ëª¨ë¸ íƒ€ìž… í™•ì¸
    with open(config_path) as f:
        config = json.load(f)
    
    # ëª¨ë¸ ìƒì„± (í›ˆë ¨ì‹œì™€ ë™ì¼í•˜ê²Œ)
    if config.get('pruning', {}).get('enabled', False):
        # Pruned ëª¨ë¸ì¸ ê²½ìš° - run_experiment.pyì™€ ë™ì¼í•œ ë°©ì‹ í•„ìš”
        import pruning.dcil
        model, _ = pruning.models.__dict__['resnet'](
            data=config['data']['dataset'],
            num_layers=config['model']['layers']
        )
    else:
        # Dense ëª¨ë¸
        model, _ = resnet(data='cifar10', num_layers=20)
    
    # DataParallel ì ìš© (í›ˆë ¨ì‹œì™€ ë™ì¼)
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(model_path, map_location='cpu')
    if 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    return model

def confidence_based_mia(model, member_loader, non_member_loader, device):
    """Confidence ê¸°ë°˜ MIA"""
    model.eval()
    
    member_confidences = []
    non_member_confidences = []
    
    with torch.no_grad():
        # Member ë°ì´í„°
        for data, target in member_loader:
            data = data.to(device)
            output = model(data)
            probs = torch.softmax(output, dim=1)
            max_probs = torch.max(probs, dim=1)[0]
            member_confidences.extend(max_probs.cpu().numpy())
        
        # Non-member ë°ì´í„°
        for data, target in non_member_loader:
            data = data.to(device)
            output = model(data)
            probs = torch.softmax(output, dim=1)
            max_probs = torch.max(probs, dim=1)[0]
            non_member_confidences.extend(max_probs.cpu().numpy())
    
    # AUC ê³„ì‚° (ê°„ë‹¨í•œ threshold ë°©ë²•)
    thresholds = np.linspace(0, 1, 1000)
    best_acc = 0
    
    for threshold in thresholds:
        member_pred = np.array(member_confidences) > threshold
        non_member_pred = np.array(non_member_confidences) <= threshold
        
        accuracy = (member_pred.sum() + non_member_pred.sum()) / (len(member_confidences) + len(non_member_confidences))
        best_acc = max(best_acc, accuracy)
    
    return {
        'member_conf_mean': float(np.mean(member_confidences)),
        'non_member_conf_mean': float(np.mean(non_member_confidences)),
        'attack_accuracy': float(best_acc),
        'member_conf_std': float(np.std(member_confidences)),
        'non_member_conf_std': float(np.std(non_member_confidences))
    }

def evaluate_model_mia(model_path, model_name, method, sparsity_percent):
    """ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•œ MIA í‰ê°€"""
    
    print(f"ðŸ” {model_name} MIA í‰ê°€ ì¤‘...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        config_path = model_path.parent / 'config.json'
        model = load_model_from_checkpoint(model_path, config_path)
        model = model.to(device)
        
        # ë°ì´í„° ì¤€ë¹„ (ê°„ë‹¨í•œ ë¶„í• )
        train_loader, test_loader = cifar10_loader(128, 4, '~/Datasets/CIFAR', cuda=torch.cuda.is_available())
        train_dataset = train_loader.dataset
        
        # Member/Non-member ë¶„í•  (í›ˆë ¨ì…‹ì˜ ì ˆë°˜ì”©)
        total_size = len(train_dataset)
        member_size = total_size // 2
        
        member_indices = list(range(member_size))
        non_member_indices = list(range(member_size, total_size))
        
        member_dataset = torch.utils.data.Subset(train_dataset, member_indices)
        non_member_dataset = torch.utils.data.Subset(train_dataset, non_member_indices)
        
        member_loader = DataLoader(member_dataset, batch_size=128, shuffle=False)
        non_member_loader = DataLoader(non_member_dataset, batch_size=128, shuffle=False)
        
        # MIA ìˆ˜í–‰
        mia_result = confidence_based_mia(model, member_loader, non_member_loader, device)
        
        # ê²°ê³¼ ë°˜í™˜ (NumPy íƒ€ìž…ì„ Python ê¸°ë³¸ íƒ€ìž…ìœ¼ë¡œ ë³€í™˜)
        result = {
            'name': model_name,
            'method': method,
            'sparsity_percent': int(sparsity_percent),
            'attack_accuracy': float(mia_result['attack_accuracy']),
            'member_confidence': float(mia_result['member_conf_mean']),
            'non_member_confidence': float(mia_result['non_member_conf_mean']),
            'confidence_gap': float(mia_result['member_conf_mean'] - mia_result['non_member_conf_mean'])
        }
        
        print(f"âœ… {model_name}: Attack Acc={mia_result['attack_accuracy']:.3f}, Gap={result['confidence_gap']:.3f}")
        return result
        
    except Exception as e:
        print(f"âŒ {model_name} MIA í‰ê°€ ì‹¤íŒ¨: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description='MIA Evaluation')
    parser.add_argument('--model_path', type=str, help='Path to model checkpoint')
    parser.add_argument('--pruning_method', type=str, help='Pruning method (static/dpf)')  
    parser.add_argument('--sparsity', type=float, help='Sparsity level')
    
    args = parser.parse_args()
    
    # If specific model path is provided, evaluate only that model
    if args.model_path:
        model_path = Path(args.model_path)
        if model_path.exists():
            # Extract method and sparsity from arguments or path
            method = args.pruning_method if args.pruning_method else 'dense'
            sparsity = args.sparsity if args.sparsity is not None else 0.0
            sparsity_percent = int(sparsity * 100)
            
            config_path = model_path.parent / 'config.json'
            if config_path.exists():
                with open(config_path) as f:
                    config = json.load(f)
                model_name = config.get('name', 'unknown_model')
            else:
                model_name = model_path.stem
            
            result = evaluate_model_mia(model_path, model_name, method, sparsity_percent)
            if result:
                # Save individual result
                result_path = model_path.parent / 'mia_results.json'
                with open(result_path, 'w') as f:
                    json.dump(result, f, indent=2)
                print(f"MIA results saved to: {result_path}")
            return
    
    # Otherwise, evaluate all models in runs directory
    runs_dir = Path('./runs')
    mia_results = []
    
    print("ðŸŽ¯ Dense vs Static vs DPF MIA í‰ê°€")
    print("=" * 50)
    
    # Dense ëª¨ë¸ë“¤
    dense_dir = runs_dir / 'dense'
    if dense_dir.exists():
        for seed_dir in dense_dir.iterdir():
            if seed_dir.is_dir():
                model_path = seed_dir / 'best_model.pth'
                if model_path.exists():
                    with open(seed_dir / 'experiment_summary.json') as f:
                        summary = json.load(f)
                    model_name = summary['hyperparameters']['name']
                    
                    result = evaluate_model_mia(model_path, model_name, 'dense', 0)
                    if result:
                        mia_results.append(result)
    
    # Static/DPF ëª¨ë¸ë“¤
    for method in ['static', 'dpf']:
        method_dir = runs_dir / method
        if method_dir.exists():
            for sparsity_dir in method_dir.iterdir():
                if sparsity_dir.is_dir() and sparsity_dir.name.startswith('sparsity'):
                    # Parse sparsity from folder name using regex
                    name = sparsity_dir.name  # ì˜ˆ: 'sparsity_0.5'
                    m = re.match(r'^sparsity[_-]?([0-9]*\.?[0-9]+)$', name)
                    if not m:
                        print(f"Warning: Unrecognized sparsity folder name: {name}, skipping")
                        continue
                    sparsity = float(m.group(1))
                    sparsity_percent = int(sparsity * 100)
                    
                    for seed_dir in sparsity_dir.iterdir():
                        if seed_dir.is_dir():
                            model_path = seed_dir / 'best_model.pth'
                            if model_path.exists():
                                with open(seed_dir / 'experiment_summary.json') as f:
                                    summary = json.load(f)
                                model_name = summary['hyperparameters']['name']
                                
                                result = evaluate_model_mia(model_path, model_name, method, sparsity_percent)
                                if result:
                                    mia_results.append(result)
    
    # MIA ê²°ê³¼ CSV ì €ìž¥
    if mia_results:
        import csv
        with open('mia_evaluation_results.csv', 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['name', 'method', 'sparsity_percent', 'attack_accuracy', 'member_confidence', 'non_member_confidence', 'confidence_gap']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(mia_results)
        
        print(f"\nðŸ“ MIA ê²°ê³¼ ì €ìž¥: mia_evaluation_results.csv")
        print(f"âœ… ì´ {len(mia_results)}ê°œ ëª¨ë¸ MIA í‰ê°€ ì™„ë£Œ!")
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\nðŸ“Š MIA ê³µê²© ì„±ê³µë¥  ìš”ì•½:")
        print(f"{'Method':<8} {'Sparsity':<8} {'Attack Acc':<12} {'Conf Gap':<10}")
        print("-" * 45)
        
        mia_results.sort(key=lambda x: (x['sparsity_percent'], x['method']))
        for result in mia_results:
            sparsity_text = f"{result['sparsity_percent']}%" if result['sparsity_percent'] > 0 else "0%"
            print(f"{result['method']:<8} {sparsity_text:<8} {result['attack_accuracy']:<12.3f} {result['confidence_gap']:<10.3f}")
    
    else:
        print("âŒ MIA í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()